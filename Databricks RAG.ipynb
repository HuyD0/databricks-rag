{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b95775-2fca-4c9b-b56d-05cda2b19cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57cdf514-046b-4d82-a1e7-8c3117a47fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = azure_client_id\n",
    "os.environ[\"AZURE_TENANT_ID\"] = azure_tenant_id\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = dbutils.secrets.get(scope=\"azure\",key=\"rag\")\n",
    "\n",
    "# 1. Initialize the credential object\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# 2. ‚≠ê Create the callable token provider using the helper function\n",
    "# The scope tells Azure what service we want to access.\n",
    "token_provider = get_bearer_token_provider(\n",
    "    credential, \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33a7b4b-6e21-4d95-86c0-258ace32ef9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "import the libraries"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# from azure.core.credentials import AzureKeyCredential # Not needed if using DefaultAzureCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.ai.agents.models import FunctionTool # Make sure this is imported!\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7119a09f-9ed1-4f90-9e9f-f4ca801e285d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instantiate the blob client to work with the files"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Config\n",
    "STORAGE_ACCOUNT_NAME =\"tfstate6666\"\n",
    "CONTAINER_NAME = \"pdfs\"\n",
    "\n",
    "storage_account_url = f\"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "\n",
    "try:\n",
    "  \n",
    "    blob_service_client = BlobServiceClient(account_url=storage_account_url, credential=credential)\n",
    "    container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "    print(f\"‚úÖ Blob service client initialized.\")\n",
    "    print(list(container_client.list_blobs()))\n",
    "except Exception as e:\n",
    "    print(f\"Error during client initialization: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2470e8-6eb8-4ab4-b7df-c5687b86e38a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define the unity catalog"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define volume, folder, and file details.\n",
    "catalog            = 'rag'\n",
    "schema             = 'development'\n",
    "volume             = 'blob'\n",
    "folder             = 'markdown'\n",
    "volume_path        = f\"/Volumes/{catalog}/{schema}/{volume}/{folder}\" # /Volumes/main/default/my-volume\n",
    "\n",
    "display(dbutils.fs.ls(volume_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "691ef363-2a32-494c-ac90-aaa09251182c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### load the files into a table for versioning with checkpointing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e4b98c-1b1a-4268-8e18-522198a7af4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "autoloader to ingest new files into a table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = (spark.readStream\n",
    "        .format('cloudFiles')\n",
    "        .option('cloudFiles.format', 'BINARYFILE')\n",
    "        .option(\"pathGlobFilter\", \"*.md\")\n",
    "        .load('dbfs:'+volume_path))\n",
    "\n",
    "# Write the data as a Delta table\n",
    "(df.writeStream\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"checkpointLocation\", f'dbfs:{volume_path}/checkpoints/')\n",
    "  .table('rag.development.md_raw').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1714670f-c124-42e0-be69-2fc08cc32223",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "view the table"
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT path, modificationTime FROM rag.development.md_raw LIMIT 2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c2bf85-a8dd-457b-8681-edaf394025ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "view the table"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT content FROM rag.development.md_raw\")\n",
    "# binary_data_column = df.select(\"content\")\n",
    "# md_binary_data = binary_data_column.collect()[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849d75af-1660-40e9-8215-d2850bd6e30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "\n",
    "# Spark/Databricks table and column names\n",
    "TABLE_NAME = \"rag.development.md_raw\"\n",
    "CONTENT_COLUMN = \"content\"  # The column with binary Markdown data\n",
    "SOURCE_COLUMN = \"path\" # An identifier column (e.g., file name, URI, or ID)\n",
    "chunk_size = os.getenv(\"CHUNK_SIZE\")\n",
    "overlap = os.getenv(\"OVERLAP\")\n",
    "\n",
    "\n",
    "# This will split the document based on Markdown headers (H1, H2, H3)\n",
    "# and add the header text to each chunk's metadata.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"PageTitle\"),\n",
    "    (\"##\", \"PageSubtitle\"),\n",
    "    (\"###\", \"PageSection\"),\n",
    "]\n",
    "\n",
    "# Initialize Text Splitter \n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# Process Markdown from the Spark DataFrame \n",
    "\n",
    "all_final_chunks = []\n",
    "rows_processed = 0\n",
    "\n",
    "print(f\"üîÑ Querying Spark table '{TABLE_NAME}'...\")\n",
    "\n",
    "try:\n",
    "    # Select the content and a source identifier column\n",
    "    # Using .toLocalIterator() is memory-efficient for large tables\n",
    "    query = f\"SELECT {CONTENT_COLUMN}, {SOURCE_COLUMN} FROM {TABLE_NAME}\"\n",
    "    df = spark.sql(query)\n",
    "    \n",
    "    print(f\"‚úÖ Query successful. Processing rows...\")\n",
    "\n",
    "    for row in df.toLocalIterator():\n",
    "        rows_processed += 1\n",
    "        binary_md_data = row[CONTENT_COLUMN]\n",
    "        source_identifier = row[SOURCE_COLUMN]\n",
    "\n",
    "        print(f\"  - Processing source: {source_identifier}\")\n",
    "\n",
    "        if not binary_md_data:\n",
    "            print(f\"    ‚ö†Ô∏è Warning: No binary data found for source '{source_identifier}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # a. ‚≠êÔ∏è Decode the binary Markdown data into a text string\n",
    "        try:\n",
    "            # Markdown is text, so we decode it (UTF-8 is standard)\n",
    "            md_text = binary_md_data.decode('utf-8')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error decoding Markdown for source '{source_identifier}'. Skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Split the text using the Markdown splitter\n",
    "        # This method returns Document objects directly, including metadata for the headers.\n",
    "        final_chunks = markdown_splitter.split_text(md_text)\n",
    "        \n",
    "        # Add the original source identifier to each chunk's metadata\n",
    "        # The splitter already created metadata with header info, so we just add to it.\n",
    "        for chunk in final_chunks:\n",
    "            chunk.metadata[\"source\"] = source_identifier\n",
    "        \n",
    "        all_final_chunks.extend(final_chunks)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå An error occurred during Spark processing: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "871b3366-8a78-4b99-9751-95655ec4313f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a table in the catalog to hold the chunks & emeddings for versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba80e81-2d1d-4e20-8d7b-f1dd44c0eada",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a table to hold the data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS rag.development.md_chunks (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY, --Need a PK\n",
    "  source STRING,\n",
    "  PageTitle STRING, \n",
    "  PageSubtitle STRING,\n",
    "  PageSection STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY <FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfacda27-763e-46e6-a5f7-ca5ba0e959b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Target the delta table\n",
    "TARGET_TABLE_NAME = \"rag.development.md_chunks\"\n",
    "\n",
    "if final_chunks:\n",
    "    print(f\"Processing {len(final_chunks)} document chunks...\")\n",
    "    credential = DefaultAzureCredential()\n",
    "    # 1. ü§ñ Generate Embeddings from Azure OpenAI\n",
    "    print(\"Generating embeddings for all chunks...\")\n",
    "    embedding_endpoint = \"https://aifoundry6666.cognitiveservices.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2023-05-15\"\n",
    "    embedding_model_name = \"text-embedding-3-large\" \n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "    embedding_client = AzureOpenAI(\n",
    "        api_version=\"2024-02-01\",\n",
    "        azure_endpoint=embedding_endpoint,\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "\n",
    "    # Extract text content and generate embeddings in a single call\n",
    "    chunks_text = [doc.page_content for doc in final_chunks]\n",
    "    embedding_response = embedding_client.embeddings.create(model=model_name, input=chunks_text)\n",
    "    embeddings = [item.embedding for item in embedding_response.data]\n",
    "    print(f\"‚úÖ Generated {len(embeddings)} embedding vectors.\")\n",
    "\n",
    "    # 2. üìù Combine chunks, metadata, and embeddings\n",
    "    # Use zip to pair each document with its corresponding embedding vector\n",
    "    data_for_df = [\n",
    "        {\n",
    "            \"page_content\": doc.page_content, \n",
    "            \"embedding\": emb,                 # <-- Added the new embedding\n",
    "            **doc.metadata\n",
    "        }\n",
    "        for doc, emb in zip(final_chunks, embeddings)\n",
    "    ]\n",
    "\n",
    "    # 3. üîÑ Create and Align the Spark DataFrame\n",
    "    print(\"Creating Spark DataFrame with embeddings...\")\n",
    "    df_chunks = spark.createDataFrame(data_for_df)\n",
    "\n",
    "    # Rename 'page_content' to 'content' to match the target Delta table schema\n",
    "    if \"page_content\" in df_chunks.columns:\n",
    "        df_chunks = df_chunks.withColumnRenamed(\"page_content\", \"content\")\n",
    "\n",
    "    # Define the columns in the desired order for the final table, now including 'embedding'\n",
    "    schema_columns = [\n",
    "        \"source\",\n",
    "        \"PageTitle\",\n",
    "        \"PageSubtitle\",\n",
    "        \"PageSection\",\n",
    "        \"content\",\n",
    "        \"embedding\"  # <-- Added embedding to the schema definition\n",
    "    ]\n",
    "    \n",
    "    # Filter the list to include only columns that exist in our DataFrame.\n",
    "    # This prevents errors if a metadata field is not present in all chunks.\n",
    "    final_columns_to_select = [col for col in schema_columns if col in df_chunks.columns]\n",
    "    \n",
    "    # Select and reorder the columns to match the table structure\n",
    "    df_to_write = df_chunks.select(final_columns_to_select)\n",
    "\n",
    "    # 4. üíæ Write the DataFrame to the Delta table\n",
    "    print(f\"Appending {df_to_write.count()} chunks with embeddings to Delta table: {TARGET_TABLE_NAME}...\")\n",
    "    \n",
    "    (df_to_write.write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\") \n",
    "      .saveAsTable(TARGET_TABLE_NAME))\n",
    "      \n",
    "    print(f\"‚úÖ Successfully saved to {TARGET_TABLE_NAME}\")\n",
    "    \n",
    "    # 5. ‚úÖ Verify the written data\n",
    "    # print(\"Verifying the written data...\")\n",
    "    # display(spark.sql(f\"SELECT source, PageTitle, content, embedding FROM {TARGET_TABLE_NAME} LIMIT 2\"))\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No chunks were generated, so no data was processed or saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55bf65cd-dba9-4b8f-a3d1-bd0647782ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Build the AI search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2246c0b-0d30-46b1-86ec-61c6934cb12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "import uuid\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from openai import AzureOpenAI\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    ScoringProfile, # Import ScoringProfile\n",
    "    TextWeights,      # Import TextWeights\n",
    ")\n",
    "\n",
    "index_name=\"ragamuffin-index\"\n",
    "    # --- Step 6: Define and Create the Search Index ---\n",
    "print(\"3. Defining and creating search index schema...\")\n",
    "index_client = SearchIndexClient(endpoint=\"https://search6666.search.windows.net\", credential=credential)\n",
    "# Use the correct dimensions for your embedding model (e.g., 3072 for text-embedding-3-large)\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=\"Edm.String\", key=True),\n",
    "    SearchableField(name=\"content\", type=\"Edm.String\", searchable=True),\n",
    "    SearchableField(name=\"source\", type=\"Edm.String\", searchable=True),\n",
    "    # Add header fields for filtering and context\n",
    "    SearchableField(name=\"page_title\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "    SearchableField(name=\"page_subtitle\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "    SearchableField(name=\"page_section\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "    SearchField(name=\"content_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, vector_search_dimensions=os.getenv(\"EMBEDDING_DIMENSIONS\"),\n",
    "                    vector_search_profile_name=\"my-hnsw-profile\")\n",
    "]\n",
    "\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    profiles=[VectorSearchProfile(name=\"my-hnsw-profile\", algorithm_configuration_name=\"my-hnsw-config\")],\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"my-hnsw-config\")]\n",
    ")\n",
    "\n",
    "# =Define the Scoring Profile to boost 'page_section' \n",
    "scoring_profile = ScoringProfile(\n",
    "    name=\"boost_section_profile\",\n",
    "    text_weights=TextWeights(\n",
    "        weights={\n",
    "            \"page_section\": 5,\n",
    "            \"page_title\": 3,\n",
    "            \"page_subtitle\": 2,\n",
    "            \"content\": 1\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "index = SearchIndex(name=index_name, \n",
    "                    fields=fields, \n",
    "                    vector_search=vector_search,\n",
    "                    scoring_profiles=[scoring_profile]\n",
    "                    )\n",
    "\n",
    "try:\n",
    "    index_client.create_index(index)\n",
    "    print(f\"   Index '{os.getenv(\"AZURE_SEARCH_INDEX_NAME\")}' created.\")\n",
    "except ResourceExistsError:\n",
    "    print(f\"   Index '{os.getenv(\"AZURE_SEARCH_INDEX_NAME\")}' already exists.\")\n",
    "\n",
    "# --- Step 7: Prepare and Upload Documents üì§ ---\n",
    "print(\"4. Preparing and uploading documents to the index...\")\n",
    "documents_to_upload = []\n",
    "for i, doc in enumerate(all_final_chunks):\n",
    "    documents_to_upload.append({\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": doc.page_content,\n",
    "        \"content_vector\": embeddings[i],\n",
    "        \"source\": doc.metadata.get(\"source\"),\n",
    "        \"page_title\": str(doc.metadata.get(\"Page Title\")), \n",
    "        \"page_subtitle\": str(doc.metadata.get(\"Page Subtitle\")),\n",
    "        \"page_section\": str(doc.metadata.get(\"Page Section\")) \n",
    "    })\n",
    "    \n",
    "search_client = SearchClient(endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"), index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"), credential=credential)\n",
    "search_client.upload_documents(documents=documents_to_upload)\n",
    "print(\"   ‚úÖ Upload complete!\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9f6d8a-fa6e-4363-abf3-ffd236d57398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use the Search SDK and track with mlflow to test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720af571-d532-4e93-ae67-600abe52e31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "import mlflow\n",
    "\n",
    "# Assume embedding_client, search_client, and model_name are already configured\n",
    "# from a previous setup.\n",
    "\n",
    "@mlflow.trace()\n",
    "def retrieve_from_azure_ai_search(query: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Performs a vector search in Azure AI Search and logs details to MLflow.\n",
    "    \"\"\"\n",
    "    AZURE_SEARCH_ENDPOINT= \"https://search6666.search.windows.net\"\n",
    "    search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "                                  index_name=AZURE_SEARCH_ENDPOINT, \n",
    "                                  credential=credential)\n",
    "    params = {\n",
    "        \"search_text\": query,\n",
    "        \"embedding_model\": model_name,\n",
    "        \"search_index_name\": index_name,\n",
    "        \"top_k\": top_k,\n",
    "        \"search_type\": \"hnsw\",\n",
    "        \"embedding_dimensions\": os.getenv(\"EMBEDDING_DIMENSIONS\"),\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"overlap\": overlap,\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "\n",
    "    # Generate the query vector\n",
    "    search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=index_name, credential=credential)\n",
    "    response = embedding_client.embeddings.create(input=query, model=model_name)\n",
    "    query_vector = response.data[0].embedding\n",
    "\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k_nearest_neighbors=top_k,\n",
    "        fields=\"content_vector\"  # The name of the vector field in your index\n",
    "    )\n",
    "\n",
    "    # Perform the vector search\n",
    "    results = search_client.search(\n",
    "        select=[\"source\",\"page_title\",\"page_subtitle\",\"page_section\", \"content\"],\n",
    "        vector_queries=[vector_query],\n",
    "        top=top_k\n",
    "    )\n",
    "\n",
    "    # Format the retrieved documents\n",
    "    retrieved_docs = [{\"source\": doc[\"source\"],\"page_title\": doc[\"page_title\"],\"page_subtitle\": doc[\"page_subtitle\"],\"page_subtitle\": doc[\"page_subtitle\"],\"content\": doc[\"content\"],  \"score\": doc[\"@search.score\"]} for doc in results]\n",
    "        # --- Log RAG Metrics ---\n",
    "    if retrieved_docs:\n",
    "        scores = [doc[\"score\"] for doc in retrieved_docs]\n",
    "        mlflow.log_metric(\"retrieved_docs_count\", len(retrieved_docs))\n",
    "        mlflow.log_metric(\"average_retrieval_score\", sum(scores) / len(scores))\n",
    "        mlflow.log_metric(\"min_retrieval_score\", min(scores))\n",
    "\n",
    "    return retrieved_docs\n",
    "\n",
    "\n",
    "# Create experiment and set tags\n",
    "mlflow.set_experiment(\"/Users/huy.d@hotmail.com/RAG_with_Azure_AI_Search_exp\")\n",
    "description = \"Evaluating retriever\"\n",
    "experiment_tags = {\n",
    "    \"project\": \"RAG\",\n",
    "    \"domain\": \"DA\",\n",
    "    \"purpose\": \"Retrieval evaluation\"\n",
    "}\n",
    "\n",
    "\n",
    "mlflow.set_experiment_tags(experiment_tags)\n",
    "\n",
    "\n",
    "# Use a context manager to ensure the run is properly managed \n",
    "with mlflow.start_run(run_name=\"Retrieval Test\"):\n",
    "    print(\"\\nPerforming tracked retrieval inside an MLflow run...\")\n",
    "    \n",
    "    retrieved_documents = retrieve_from_azure_ai_search(\n",
    "        query=\"what is a ragamuffin\",\n",
    "        top_k=5\n",
    "    )\n",
    "\n",
    "    # print(\"\\nRetrieved Documents:\")\n",
    "    # for doc in retrieved_documents:\n",
    "    #     print(f\"  page_section: {doc.get('page_section')}, Score: {doc.get('score'):.4f}, Text: {doc.get('content')}\")\n",
    "\n",
    "print(\"\\n‚úÖ Retrieval process tracked in MLflow. Run 'mlflow ui' to view the trace.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dee53aff-42a8-4c97-9494-639ed7de231e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Build out the RAG Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba69717-842b-4e57-82db-989009176c3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from mlflow.models import ModelSignature\n",
    "from mlflow.types import Schema, ColSpec\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- Configuration ---\n",
    "# Values are now loaded from the .env file\n",
    "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_SEARCH_ENDPOINT = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "AZURE_SEARCH_INDEX_NAME = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "SEARCH_TYPE=os.getenv(\"SEARCH_TYPE\")\n",
    "CHAT_MODEL_DEPLOYMENT = os.getenv(\"CHAT_MODEL_DEPLOYMENT\")\n",
    "EMBEDDING_MODEL_NAME = os.getenv(\"EMBEDDING_MODEL_NAME\")\n",
    "EMEDDING_ENDPOINT=os.getenv(\"EMBEDDING_ENDPOINT\")\n",
    "OPENAI_API_VERSION = os.getenv(\"OPENAI_API_VERSION\")\n",
    "GENERATION_TEMPERATURE=os.getenv(\"GENERATION_TEMPERATURE\")\n",
    "MAX_TOKENS=os.getenv(\"MAX_TOKENS\")\n",
    "CHUNK_SIZE=os.getenv(\"CHUNK_SIZE\")\n",
    "OVERLAP=os.getenv(\"OVERLAP\")\n",
    "EMBEDDING_DIMENSIONS=os.getenv(\"EMBEDDING_DIMENSIONS\")\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = azure_client_id\n",
    "os.environ[\"AZURE_TENANT_ID\"] = azure_tenant_id\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = dbutils.secrets.get(scope=\"azure\",key=\"rag\")\n",
    "# Define the Delta table used as the source for the RAG knowledge base\n",
    "SOURCE_DELTA_TABLE = os.getenv(\"SOURCE_DELTA_TABLE\")\n",
    "SEARCH_TYPE=os.getenv(\"SEARCH_TYPE\")\n",
    "# --- MLflow Setup ---\n",
    "MLFLOW_EXPERIMENT_PATH = os.getenv(\"MLFLOW_EXPERIMENT_PATH\")\n",
    "mlflow.set_experiment(MLFLOW_EXPERIMENT_PATH)\n",
    "print(MAX_TOKENS)\n",
    "# ‚ú® NEW: Define constants for logging\n",
    "SYSTEM_PROMPT = \"You are an intelligent assistant. Use the context provided to answer the user's question.\"\n",
    "RETRIEVAL_FIELDS = [\"source\", \"page_title\", \"content\"]\n",
    "\n",
    "# --- Client Initialization (for experiment run) ---\n",
    "credential = DefaultAzureCredential()\n",
    "token_provider = get_bearer_token_provider(credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "aoai_client = AzureOpenAI(\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token_provider=token_provider,\n",
    ")\n",
    "embedding_client = AzureOpenAI(\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_endpoint=EMEDDING_ENDPOINT,\n",
    "    azure_ad_token_provider=token_provider,\n",
    ")\n",
    "search_client = SearchClient(\n",
    "    endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "    index_name=AZURE_SEARCH_INDEX_NAME,\n",
    "    credential=credential\n",
    ")\n",
    "\n",
    "# --- RAG Functions (for tracing during experiment) ---\n",
    "def retrieve_documents(query, top_k):\n",
    "    embedding_response = embedding_client.embeddings.create(model=EMBEDDING_MODEL_NAME, input=query)\n",
    "    query_vector = embedding_response.data[0].embedding\n",
    "    vector_query = VectorizedQuery(vector=query_vector, k_nearest_neighbors=top_k, fields=\"content_vector\")\n",
    "\n",
    "    results = search_client.search(\n",
    "        select=RETRIEVAL_FIELDS, # ‚ú® UPDATED: Use the constant\n",
    "        vector_queries=[vector_query],\n",
    "        top=top_k\n",
    "    )\n",
    "    return [{\"source\": res[\"source\"], \"page_title\": res[\"page_title\"], \"content\": res[\"content\"]} for res in results]\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in retrieved_docs])\n",
    "    # ‚ú® UPDATED: system_message is now a constant\n",
    "    user_message = f\"CONTEXT:\\n---\\n{context}\\n---\\nQUESTION: {query}\"\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "    chat_response = aoai_client.chat.completions.create(\n",
    "        model=CHAT_MODEL_DEPLOYMENT,\n",
    "        messages=messages,\n",
    "        temperature=float(GENERATION_TEMPERATURE),\n",
    "        max_tokens=int(MAX_TOKENS)\n",
    "    )\n",
    "    token_usage = chat_response.usage\n",
    "    return chat_response.choices[0].message.content, user_message, token_usage\n",
    "\n",
    "@mlflow.trace()\n",
    "def run_rag_pipeline_traced(query, top_k):\n",
    "    \"\"\"Traced function for detailed pipeline analysis.\"\"\"\n",
    "    with mlflow.start_span(\"retrieval\") as span:\n",
    "        start_time = time.time()\n",
    "        documents = retrieve_documents(query, top_k)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        span.set_outputs({\"documents\": documents})\n",
    "\n",
    "    with mlflow.start_span(\"generation\") as span:\n",
    "        start_time = time.time()\n",
    "        final_answer, augmented_prompt, token_usage = generate_answer(query, documents)\n",
    "        generation_time = time.time() - start_time\n",
    "        span.set_outputs({\"answer\": final_answer})\n",
    "\n",
    "    return final_answer, documents, augmented_prompt, retrieval_time, generation_time, token_usage\n",
    "\n",
    "# --- Main Experiment and Registration Run ---\n",
    "user_query = \"What are ragamuffins?\"\n",
    "top_k_value = 5\n",
    "\n",
    "with mlflow.start_run(run_name=\"RAG Experiment and Model Registration\") as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"üöÄ Starting MLflow run: {run.info.run_name} ({run_id})\")\n",
    "    print(f\"Linking source dataset: {SOURCE_DELTA_TABLE}\")\n",
    "\n",
    "    source_dataset = mlflow.data.load_delta(\n",
    "        table_name=SOURCE_DELTA_TABLE # Pass the version here\n",
    "    )\n",
    "\n",
    "    # 3. Log the dataset as an input to the run for traceability\n",
    "    mlflow.log_input(source_dataset, context=\"source_documents\")\n",
    "\n",
    "    # === Part 1: Run Experiment & Log Results ===\n",
    "    # ‚ú® UPDATED: Added new parameters to the log\n",
    "    mlflow.log_params({\n",
    "        \"top_k\": top_k_value,\n",
    "        \"embedding_model\": EMBEDDING_MODEL_NAME,\n",
    "        \"chat_model\": CHAT_MODEL_DEPLOYMENT,\n",
    "        \"search_index_name\": AZURE_SEARCH_INDEX_NAME,\n",
    "        \"system_prompt\": SYSTEM_PROMPT,\n",
    "        \"search_type\": SEARCH_TYPE,\n",
    "        \"chunk_size\":CHUNK_SIZE,\n",
    "        \"overlap\":OVERLAP,\n",
    "        \"embedding_dimensions\": EMBEDDING_DIMENSIONS,\n",
    "        \"temperature\": GENERATION_TEMPERATURE,\n",
    "        \"retrieval_fields\": RETRIEVAL_FIELDS\n",
    "    })\n",
    "\n",
    "    # Execute the traced pipeline\n",
    "    final_answer, docs, prompt, ret_time, gen_time, tokens = run_rag_pipeline_traced(\n",
    "        query=user_query,\n",
    "        top_k=top_k_value\n",
    "    )\n",
    "\n",
    "    # Log metrics and artifacts\n",
    "    mlflow.log_metrics({\n",
    "        \"retrieval_time_sec\": round(ret_time, 2),\n",
    "        \"generation_time_sec\": round(gen_time, 2),\n",
    "        \"total_time_sec\": round(ret_time + gen_time, 2),\n",
    "        \"prompt_tokens\": tokens.prompt_tokens,\n",
    "        \"completion_tokens\": tokens.completion_tokens,\n",
    "        \"total_tokens\": tokens.total_tokens,\n",
    "    })\n",
    "\n",
    "    rag_table = pd.DataFrame({\n",
    "        \"prompt\": [user_query],\n",
    "        \"augmented_prompt\": [prompt],\n",
    "        \"final_answer\": [final_answer],\n",
    "        \"retrieved_documents\": [str(docs)]\n",
    "    })\n",
    "    mlflow.log_table(data=rag_table, artifact_file=\"rag_results.json\")\n",
    "\n",
    "    print(\"‚úÖ Experiment results logged.\")\n",
    "    print(f\"\\nFinal Answer:\\n{final_answer}\\n\")\n",
    "\n",
    "    # === Part 2: Log and Register the Self-Contained Model ===\n",
    "    print(\"üé¨ Starting model logging and registration...\")\n",
    "\n",
    "    # Define the model signature\n",
    "    input_schema = Schema([ColSpec(\"string\", \"question\"), ColSpec(\"long\", \"top_k\")])\n",
    "    output_schema = Schema([\n",
    "    ColSpec(\"string\", \"answer\"), \n",
    "    ColSpec(\"string\", \"retrieved_documents\")])\n",
    "    signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "\n",
    "    # Define an input example\n",
    "    input_example = pd.DataFrame({\"question\": [user_query], \"top_k\": [top_k_value]})\n",
    "\n",
    "    # Define pip requirements for the model environment\n",
    "    pip_requirements = [\n",
    "        \"mlflow>=2.10\",\n",
    "        \"pandas\",\n",
    "        \"openai>=1.12.0\",\n",
    "        \"azure-identity>=1.15.0\",\n",
    "        \"azure-search-documents>=11.4.0\"\n",
    "    ]\n",
    "\n",
    "    # Log the model using the code from rag_model.py\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"rag_model\",\n",
    "        python_model=\"rag_model.py\", # Log an instance of the class\n",
    "        input_example=input_example,\n",
    "        pip_requirements=pip_requirements,\n",
    "        signature=signature\n",
    "    )\n",
    "\n",
    "    print(\"üì¶ Model logged successfully.\")\n",
    "\n",
    "    # Register the logged model to the Unity Catalog Model Registry\n",
    "    print(\"üñãÔ∏è Registering model in the Model Registry...\")\n",
    "    registered_model = mlflow.register_model(\n",
    "        model_uri=model_info.model_uri,\n",
    "        name=\"rag.development.rag_model\" # UC 3-level name: catalog.schema.model\n",
    "    )\n",
    "    print(f\"‚úÖ Model '{registered_model.name}' version {registered_model.version} registered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "177f38b9-2f69-4425-997d-0507851a2e49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "use the registered model"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "import pandas as pd\n",
    "\n",
    "# Load the registered model\n",
    "model_name = \"rag.development.rag_model\"\n",
    "model_version = \"10\"  # Specify the version you want to use\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "# Load the model as a PyFunc model\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Define the input data\n",
    "input_data = pd.DataFrame({\n",
    "    \"question\": [\"What are ragamuffins?\"],\n",
    "    \"top_k\": [5]\n",
    "})\n",
    "\n",
    "# Use the model for inference\n",
    "predictions = loaded_model.predict(input_data)\n",
    "\n",
    "# Display the predictions\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6add28fb-33eb-4824-aaf0-08860cef4397",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "serve the model"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedModelInput\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# 1. Define the endpoint and model details\n",
    "endpoint_name = \"rag-chatbot-endpoint\"\n",
    "model_name = \"rag.development.rag_model\"\n",
    "model_version = \"10\"\n",
    "\n",
    "\n",
    "env_vars = {\n",
    "    # --- Secrets fetched from Databricks Secret Scope 'azure' ---\n",
    "    \"AZURE_CLIENT_ID\":       \"7318b99c-c3ab-483e-979f-34c7e6bad8ea\",\n",
    "    \"AZURE_TENANT_ID\":       \"7f6a2cf9-5e4e-46ae-95d4-74016c1df1a6\",\n",
    "    \"AZURE_CLIENT_SECRET\":   \"{{secrets/azure/rag}}\",\n",
    "\n",
    "    # --- Azure Service Endpoints & Versions (as plain text) ---\n",
    "    \"OPENAI_API_VERSION\":      \"2024-02-01\",\n",
    "    \"AZURE_OPENAI_ENDPOINT\":   \"https://aifoundry6666.openai.azure.com/\",\n",
    "    \"AZURE_SEARCH_ENDPOINT\":   \"https://search6666.search.windows.net\",\n",
    "    \"AZURE_SEARCH_INDEX_NAME\": \"ragamuffin-index\",\n",
    "\n",
    "    # --- Model & Embedding Configuration (as plain text) ---\n",
    "    \"EMBEDDING_MODEL_NAME\":    \"text-embedding-3-large\",\n",
    "    \"CHAT_MODEL_DEPLOYMENT\":   \"gpt-4.1-mini\",\n",
    "    \"SYSTEM_MESSAGE_PROMPT\":   \"You are an intelligent assistant...\",\n",
    "    \"TOP_K\":                   \"5\",\n",
    "    \"GENERATION_TEMPERATURE\":  \"0.1\"\n",
    "}\n",
    "\n",
    "# This specifies which model version to deploy, the workload size, and scaling.\n",
    "served_model = ServedModelInput(\n",
    "    model_name=model_name,\n",
    "    model_version=model_version,\n",
    "    workload_size=\"Small\",  # Options: \"Small\", \"Medium\", \"Large\"\n",
    "    scale_to_zero_enabled=True, # Recommended to save costs\n",
    "    environment_vars=env_vars\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# An endpoint can serve multiple models, so the config takes a list.\n",
    "endpoint_config = EndpointCoreConfigInput(\n",
    "    served_models=[served_model]\n",
    ")\n",
    "\n",
    "# 4. Create the endpoint and wait for it to be ready\n",
    "print(f\"Creating or updating endpoint: {endpoint_name}\")\n",
    "w.serving_endpoints.create_and_wait(\n",
    "    name=endpoint_name,\n",
    "    config=endpoint_config\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Endpoint '{endpoint_name}' is now active and serving version {model_version}.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "openai",
     "azure-identity"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6578920358797710,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks RAG",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
