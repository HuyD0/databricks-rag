{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b95775-2fca-4c9b-b56d-05cda2b19cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849882d6-980f-4807-9c22-dc4106099487",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "install required libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-ai-inference azure-identity semantic-kernel flask azure-ai-documentintelligence pandas azure-storage-blob langchain langchain-community langchain-openai langchainhub openai azure-search-documents mflow azure-ai-inference azure-ai-ml databricks-sdk mlflow databricks-agents --quiet\n",
    "dbutils.library.restartPython()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57cdf514-046b-4d82-a1e7-8c3117a47fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_CLIENT_ID\"] = \"7318b99c-c3ab-483e-979f-34c7e6bad8ea\"\n",
    "os.environ[\"AZURE_TENANT_ID\"] = \"7f6a2cf9-5e4e-46ae-95d4-74016c1df1a6\"\n",
    "os.environ[\"AZURE_CLIENT_SECRET\"] = dbutils.secrets.get(scope=\"azure\",key=\"rag\")\n",
    "\n",
    "# 1. Initialize the credential object\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "# 2. ⭐ Create the callable token provider using the helper function\n",
    "# The scope tells Azure what service we want to access.\n",
    "token_provider = get_bearer_token_provider(\n",
    "    credential, \"https://cognitiveservices.azure.com/.default\"\n",
    ")\n",
    "\n",
    "# 3. Initialize the client with the callable token provider\n",
    "azure_openai_endpoint = \"https://aifoundry6666.openai.azure.com/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=azure_openai_endpoint,\n",
    "    api_version=\"2024-02-01\",\n",
    "    azure_ad_token_provider=token_provider  # Pass the new callable provider here\n",
    ")\n",
    "\n",
    "print(\"✅ Successfully initialized AzureOpenAI client.\")\n",
    "\n",
    "# 4. Define your model deployment name\n",
    "deployment_name = \"gpt-4.1-mini\"  # Make sure this deployment exists in your resource\n",
    "\n",
    "# 5. Construct the messages list\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the primary function of a CPU in a computer?\"}\n",
    "]\n",
    "\n",
    "# 6. Send the chat completion request\n",
    "try:\n",
    "    print(f\"Sending request to deployment '{deployment_name}'...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment_name,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=800\n",
    "    )\n",
    "\n",
    "    # Extract and print the response\n",
    "    final_answer = response.choices[0].message.content\n",
    "    print(\"\\nAssistant's Answer:\")\n",
    "    print(final_answer)\n",
    "\n",
    "    # Print token usage\n",
    "    print(\"\\n--- Token Usage ---\")\n",
    "    print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "    print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "    print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33a7b4b-6e21-4d95-86c0-258ace32ef9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "import the libraries"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# from azure.core.credentials import AzureKeyCredential # Not needed if using DefaultAzureCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.ai.agents.models import FunctionTool # Make sure this is imported!\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from langchain import hub\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7119a09f-9ed1-4f90-9e9f-f4ca801e285d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instantiate the blob client to work with the files"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Config\n",
    "STORAGE_ACCOUNT_NAME =\"tfstate6666\"\n",
    "CONTAINER_NAME = \"pdfs\"\n",
    "\n",
    "storage_account_url = f\"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "\n",
    "try:\n",
    "  \n",
    "    blob_service_client = BlobServiceClient(account_url=storage_account_url, credential=credential)\n",
    "    container_client = blob_service_client.get_container_client(CONTAINER_NAME)\n",
    "    print(f\"✅ Blob service client initialized.\")\n",
    "    print(list(container_client.list_blobs()))\n",
    "except Exception as e:\n",
    "    print(f\"Error during client initialization: {e}\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2470e8-6eb8-4ab4-b7df-c5687b86e38a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define the unity catalog"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define volume, folder, and file details.\n",
    "catalog            = 'rag'\n",
    "schema             = 'development'\n",
    "volume             = 'blob'\n",
    "folder             = 'markdown'\n",
    "volume_path        = f\"/Volumes/{catalog}/{schema}/{volume}/{folder}\" # /Volumes/main/default/my-volume\n",
    "\n",
    "display(dbutils.fs.ls(volume_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691ef363-2a32-494c-ac90-aaa09251182c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### load the files into a table for versioning with checkpointing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e4b98c-1b1a-4268-8e18-522198a7af4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "autoloader to ingest new files into a table"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = (spark.readStream\n",
    "        .format('cloudFiles')\n",
    "        .option('cloudFiles.format', 'BINARYFILE')\n",
    "        .option(\"pathGlobFilter\", \"*.md\")\n",
    "        .load('dbfs:'+volume_path))\n",
    "\n",
    "# Write the data as a Delta table\n",
    "(df.writeStream\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"checkpointLocation\", f'dbfs:{volume_path}/checkpoints/')\n",
    "  .table('rag.development.md_raw').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1714670f-c124-42e0-be69-2fc08cc32223",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "view the table"
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "SELECT path, modificationTime FROM rag.development.md_raw LIMIT 2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c2bf85-a8dd-457b-8681-edaf394025ea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "view the table"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT content FROM rag.development.md_raw\")\n",
    "# binary_data_column = df.select(\"content\")\n",
    "# md_binary_data = binary_data_column.collect()[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "849d75af-1660-40e9-8215-d2850bd6e30b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "\n",
    "# Spark/Databricks table and column names\n",
    "TABLE_NAME = \"rag.development.md_raw\"\n",
    "CONTENT_COLUMN = \"content\"  # The column with binary Markdown data\n",
    "SOURCE_COLUMN = \"path\" # An identifier column (e.g., file name, URI, or ID)\n",
    "chunk_size = 1000\n",
    "overlap = 100\n",
    "\n",
    "\n",
    "# This will split the document based on Markdown headers (H1, H2, H3)\n",
    "# and add the header text to each chunk's metadata.\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"PageTitle\"),\n",
    "    (\"##\", \"PageSubtitle\"),\n",
    "    (\"###\", \"PageSection\"),\n",
    "]\n",
    "\n",
    "# Initialize Text Splitter \n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "# Process Markdown from the Spark DataFrame \n",
    "\n",
    "all_final_chunks = []\n",
    "rows_processed = 0\n",
    "\n",
    "print(f\"🔄 Querying Spark table '{TABLE_NAME}'...\")\n",
    "\n",
    "try:\n",
    "    # Select the content and a source identifier column\n",
    "    # Using .toLocalIterator() is memory-efficient for large tables\n",
    "    query = f\"SELECT {CONTENT_COLUMN}, {SOURCE_COLUMN} FROM {TABLE_NAME}\"\n",
    "    df = spark.sql(query)\n",
    "    \n",
    "    print(f\"✅ Query successful. Processing rows...\")\n",
    "\n",
    "    for row in df.toLocalIterator():\n",
    "        rows_processed += 1\n",
    "        binary_md_data = row[CONTENT_COLUMN]\n",
    "        source_identifier = row[SOURCE_COLUMN]\n",
    "\n",
    "        print(f\"  - Processing source: {source_identifier}\")\n",
    "\n",
    "        if not binary_md_data:\n",
    "            print(f\"    ⚠️ Warning: No binary data found for source '{source_identifier}'. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # a. ⭐️ Decode the binary Markdown data into a text string\n",
    "        try:\n",
    "            # Markdown is text, so we decode it (UTF-8 is standard)\n",
    "            md_text = binary_md_data.decode('utf-8')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"    ❌ Error decoding Markdown for source '{source_identifier}'. Skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Split the text using the Markdown splitter\n",
    "        # This method returns Document objects directly, including metadata for the headers.\n",
    "        final_chunks = markdown_splitter.split_text(md_text)\n",
    "        \n",
    "        # Add the original source identifier to each chunk's metadata\n",
    "        # The splitter already created metadata with header info, so we just add to it.\n",
    "        for chunk in final_chunks:\n",
    "            chunk.metadata[\"source\"] = source_identifier\n",
    "        \n",
    "        all_final_chunks.extend(final_chunks)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during Spark processing: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "871b3366-8a78-4b99-9751-95655ec4313f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a table in the catalog to hold the chunks & emeddings for versioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba80e81-2d1d-4e20-8d7b-f1dd44c0eada",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create a table to hold the data"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS rag.development.md_chunks (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY, --Need a PK\n",
    "  source STRING,\n",
    "  PageTitle STRING, \n",
    "  PageSubtitle STRING,\n",
    "  PageSection STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY <FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfacda27-763e-46e6-a5f7-ca5ba0e959b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Target the delta table\n",
    "TARGET_TABLE_NAME = \"rag.development.md_chunks\"\n",
    "\n",
    "if final_chunks:\n",
    "    print(f\"Processing {len(final_chunks)} document chunks...\")\n",
    "    credential = DefaultAzureCredential()\n",
    "    # 1. 🤖 Generate Embeddings from Azure OpenAI\n",
    "    print(\"Generating embeddings for all chunks...\")\n",
    "    embedding_endpoint = \"https://aifoundry6666.cognitiveservices.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2023-05-15\"\n",
    "    embedding_model_name = \"text-embedding-3-large\" \n",
    "    token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "    embedding_client = AzureOpenAI(\n",
    "        api_version=\"2024-02-01\",\n",
    "        azure_endpoint=embedding_endpoint,\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "\n",
    "    # Extract text content and generate embeddings in a single call\n",
    "    chunks_text = [doc.page_content for doc in final_chunks]\n",
    "    embedding_response = embedding_client.embeddings.create(model=model_name, input=chunks_text)\n",
    "    embeddings = [item.embedding for item in embedding_response.data]\n",
    "    print(f\"✅ Generated {len(embeddings)} embedding vectors.\")\n",
    "\n",
    "    # 2. 📝 Combine chunks, metadata, and embeddings\n",
    "    # Use zip to pair each document with its corresponding embedding vector\n",
    "    data_for_df = [\n",
    "        {\n",
    "            \"page_content\": doc.page_content, \n",
    "            \"embedding\": emb,                 # <-- Added the new embedding\n",
    "            **doc.metadata\n",
    "        }\n",
    "        for doc, emb in zip(final_chunks, embeddings)\n",
    "    ]\n",
    "\n",
    "    # 3. 🔄 Create and Align the Spark DataFrame\n",
    "    print(\"Creating Spark DataFrame with embeddings...\")\n",
    "    df_chunks = spark.createDataFrame(data_for_df)\n",
    "\n",
    "    # Rename 'page_content' to 'content' to match the target Delta table schema\n",
    "    if \"page_content\" in df_chunks.columns:\n",
    "        df_chunks = df_chunks.withColumnRenamed(\"page_content\", \"content\")\n",
    "\n",
    "    # Define the columns in the desired order for the final table, now including 'embedding'\n",
    "    schema_columns = [\n",
    "        \"source\",\n",
    "        \"PageTitle\",\n",
    "        \"PageSubtitle\",\n",
    "        \"PageSection\",\n",
    "        \"content\",\n",
    "        \"embedding\"  # <-- Added embedding to the schema definition\n",
    "    ]\n",
    "    \n",
    "    # Filter the list to include only columns that exist in our DataFrame.\n",
    "    # This prevents errors if a metadata field is not present in all chunks.\n",
    "    final_columns_to_select = [col for col in schema_columns if col in df_chunks.columns]\n",
    "    \n",
    "    # Select and reorder the columns to match the table structure\n",
    "    df_to_write = df_chunks.select(final_columns_to_select)\n",
    "\n",
    "    # 4. 💾 Write the DataFrame to the Delta table\n",
    "    print(f\"Appending {df_to_write.count()} chunks with embeddings to Delta table: {TARGET_TABLE_NAME}...\")\n",
    "    \n",
    "    (df_to_write.write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"append\")\n",
    "      .option(\"mergeSchema\", \"true\") \n",
    "      .saveAsTable(TARGET_TABLE_NAME))\n",
    "      \n",
    "    print(f\"✅ Successfully saved to {TARGET_TABLE_NAME}\")\n",
    "    \n",
    "    # 5. ✅ Verify the written data\n",
    "    # print(\"Verifying the written data...\")\n",
    "    # display(spark.sql(f\"SELECT source, PageTitle, content, embedding FROM {TARGET_TABLE_NAME} LIMIT 2\"))\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ No chunks were generated, so no data was processed or saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55bf65cd-dba9-4b8f-a3d1-bd0647782ddf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Build the AI search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2246c0b-0d30-46b1-86ec-61c6934cb12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "import uuid\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "from openai import AzureOpenAI\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    ScoringProfile, # Import ScoringProfile\n",
    "    TextWeights,      # Import TextWeights\n",
    ")\n",
    "\n",
    "index_name=\"ragamuffin-index\"\n",
    "    # --- Step 6: Define and Create the Search Index ---\n",
    "print(\"3. Defining and creating search index schema...\")\n",
    "index_client = SearchIndexClient(endpoint=\"https://search6666.search.windows.net\", credential=credential)\n",
    "# Use the correct dimensions for your embedding model (e.g., 3072 for text-embedding-3-large)\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=\"Edm.String\", key=True),\n",
    "    SearchableField(name=\"content\", type=\"Edm.String\", searchable=True),\n",
    "    SearchableField(name=\"source\", type=\"Edm.String\", searchable=True),\n",
    "    # Add header fields for filtering and context\n",
    "    SearchableField(name=\"page_title\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "    SearchableField(name=\"page_subtitle\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "    SearchableField(name=\"page_section\", type=\"Edm.String\", filterable=True, facetable=True),\n",
    "    SearchField(name=\"content_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                    searchable=True, vector_search_dimensions=os.getenv(\"EMBEDDING_DIMENSIONS\"),\n",
    "                    vector_search_profile_name=\"my-hnsw-profile\")\n",
    "]\n",
    "\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    profiles=[VectorSearchProfile(name=\"my-hnsw-profile\", algorithm_configuration_name=\"my-hnsw-config\")],\n",
    "    algorithms=[HnswAlgorithmConfiguration(name=\"my-hnsw-config\")]\n",
    ")\n",
    "\n",
    "# =Define the Scoring Profile to boost 'page_section' \n",
    "scoring_profile = ScoringProfile(\n",
    "    name=\"boost_section_profile\",\n",
    "    text_weights=TextWeights(\n",
    "        weights={\n",
    "            \"page_section\": 5,\n",
    "            \"page_title\": 3,\n",
    "            \"page_subtitle\": 2,\n",
    "            \"content\": 1\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "index = SearchIndex(name=index_name, \n",
    "                    fields=fields, \n",
    "                    vector_search=vector_search,\n",
    "                    scoring_profiles=[scoring_profile]\n",
    "                    )\n",
    "\n",
    "try:\n",
    "    index_client.create_index(index)\n",
    "    print(f\"   Index '{os.getenv(\"AZURE_SEARCH_INDEX_NAME\")}' created.\")\n",
    "except ResourceExistsError:\n",
    "    print(f\"   Index '{os.getenv(\"AZURE_SEARCH_INDEX_NAME\")}' already exists.\")\n",
    "\n",
    "# --- Step 7: Prepare and Upload Documents 📤 ---\n",
    "print(\"4. Preparing and uploading documents to the index...\")\n",
    "documents_to_upload = []\n",
    "for i, doc in enumerate(all_final_chunks):\n",
    "    documents_to_upload.append({\n",
    "        \"id\": str(uuid.uuid4()),\n",
    "        \"content\": doc.page_content,\n",
    "        \"content_vector\": embeddings[i],\n",
    "        \"source\": doc.metadata.get(\"source\"),\n",
    "        \"page_title\": str(doc.metadata.get(\"Page Title\")), \n",
    "        \"page_subtitle\": str(doc.metadata.get(\"Page Subtitle\")),\n",
    "        \"page_section\": str(doc.metadata.get(\"Page Section\")) \n",
    "    })\n",
    "    \n",
    "search_client = SearchClient(endpoint=os.getenv(\"AZURE_SEARCH_ENDPOINT\"), index_name=os.getenv(\"AZURE_SEARCH_INDEX_NAME\"), credential=credential)\n",
    "search_client.upload_documents(documents=documents_to_upload)\n",
    "print(\"   ✅ Upload complete!\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9f6d8a-fa6e-4363-abf3-ffd236d57398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Use the Search SDK and track with mlflow to test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720af571-d532-4e93-ae67-600abe52e31c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.search.documents.models import VectorizedQuery\n",
    "import mlflow\n",
    "\n",
    "# Assume embedding_client, search_client, and model_name are already configured\n",
    "# from a previous setup.\n",
    "\n",
    "@mlflow.trace()\n",
    "def retrieve_from_azure_ai_search(query: str, top_k: int = 3):\n",
    "    \"\"\"\n",
    "    Performs a vector search in Azure AI Search and logs details to MLflow.\n",
    "    \"\"\"\n",
    "    AZURE_SEARCH_ENDPOINT= \"https://search6666.search.windows.net\"\n",
    "    search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT,\n",
    "                                  index_name=AZURE_SEARCH_ENDPOINT, \n",
    "                                  credential=credential)\n",
    "    params = {\n",
    "        \"search_text\": query,\n",
    "        \"embedding_model\": model_name,\n",
    "        \"search_index_name\": index_name,\n",
    "        \"top_k\": top_k,\n",
    "        \"search_type\": \"hnsw\",\n",
    "        \"embedding_dimensions\": os.getenv(\"EMBEDDING_DIMENSIONS\"),\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"overlap\": overlap,\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "\n",
    "    # Generate the query vector\n",
    "    search_client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=index_name, credential=credential)\n",
    "    response = embedding_client.embeddings.create(input=query, model=model_name)\n",
    "    query_vector = response.data[0].embedding\n",
    "\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=query_vector,\n",
    "        k_nearest_neighbors=top_k,\n",
    "        fields=\"content_vector\"  # The name of the vector field in your index\n",
    "    )\n",
    "\n",
    "    # Perform the vector search\n",
    "    results = search_client.search(\n",
    "        select=[\"source\",\"page_title\",\"page_subtitle\",\"page_section\", \"content\"],\n",
    "        vector_queries=[vector_query],\n",
    "        top=top_k\n",
    "    )\n",
    "\n",
    "    # Format the retrieved documents\n",
    "    retrieved_docs = [{\"source\": doc[\"source\"],\"page_title\": doc[\"page_title\"],\"page_subtitle\": doc[\"page_subtitle\"],\"page_subtitle\": doc[\"page_subtitle\"],\"content\": doc[\"content\"],  \"score\": doc[\"@search.score\"]} for doc in results]\n",
    "        # --- Log RAG Metrics ---\n",
    "    if retrieved_docs:\n",
    "        scores = [doc[\"score\"] for doc in retrieved_docs]\n",
    "        mlflow.log_metric(\"retrieved_docs_count\", len(retrieved_docs))\n",
    "        mlflow.log_metric(\"average_retrieval_score\", sum(scores) / len(scores))\n",
    "        mlflow.log_metric(\"min_retrieval_score\", min(scores))\n",
    "\n",
    "    return retrieved_docs\n",
    "\n",
    "\n",
    "# Create experiment and set tags\n",
    "mlflow.set_experiment(\"/Users/huy.d@hotmail.com/RAG_with_Azure_AI_Search_exp\")\n",
    "description = \"Evaluating retriever\"\n",
    "experiment_tags = {\n",
    "    \"project\": \"RAG\",\n",
    "    \"domain\": \"DA\",\n",
    "    \"purpose\": \"Retrieval evaluation\"\n",
    "}\n",
    "\n",
    "\n",
    "mlflow.set_experiment_tags(experiment_tags)\n",
    "\n",
    "\n",
    "# Use a context manager to ensure the run is properly managed \n",
    "with mlflow.start_run(run_name=\"Retrieval Test\"):\n",
    "    print(\"\\nPerforming tracked retrieval inside an MLflow run...\")\n",
    "    \n",
    "    retrieved_documents = retrieve_from_azure_ai_search(\n",
    "        query=\"what is a ragamuffin\",\n",
    "        top_k=5\n",
    "    )\n",
    "\n",
    "    # print(\"\\nRetrieved Documents:\")\n",
    "    # for doc in retrieved_documents:\n",
    "    #     print(f\"  page_section: {doc.get('page_section')}, Score: {doc.get('score'):.4f}, Text: {doc.get('content')}\")\n",
    "\n",
    "print(\"\\n✅ Retrieval process tracked in MLflow. Run 'mlflow ui' to view the trace.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dee53aff-42a8-4c97-9494-639ed7de231e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Build out the rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a48c05f-1478-4533-8905-ef3140b93ecf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import mlflow\n",
    "import pandas as pd \n",
    "from openai import AzureOpenAI\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential,get_bearer_token_provider\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "\n",
    "# MLflow experiment setup\n",
    "mlflow.set_experiment(\"/Users/huy.d@hotmail.com/RAG_with_Azure_AI_Search_exp\")\n",
    "\n",
    "description = \"Evaluating RAG performance\"\n",
    "\n",
    "experiment_tags = {\n",
    "    \"project\": \"RAG\",\n",
    "    \"domain\": \"DA\",\n",
    "    \"purpose\": \"Retrieval evaluation\"\n",
    "}\n",
    "\n",
    "OPENAI_API_VERSION=\"2024-02-01\"\n",
    "AZURE_OPENAI_ENDPOINT=\"https://aifoundry6666.cognitiveservices.azure.com/openai/deployments/gpt-4.1-mini/chat/completions?api-version=2025-01-01-preview\"\n",
    "AZURE_SEARCH_ENDPOINT=\"https://search6666.search.windows.net\"\n",
    "EMBEDDING_DIMENSIONS = 3072\n",
    "EMBEDDING_MODEL_NAME=\"text-embedding-3-large\"\n",
    "chat_model_deployment=\"gpt-4.1-mini\"\n",
    "\n",
    "mlflow.set_experiment_tags(experiment_tags)\n",
    "mlflow.set_active_model(name=\"rag-dev\")\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "cognitive_services_scope = \"https://cognitiveservices.azure.com/.default\"\n",
    "token_provider = get_bearer_token_provider(\n",
    "    credential, \n",
    "    cognitive_services_scope\n",
    ")\n",
    "\n",
    "#clients\n",
    "aoai_client = AzureOpenAI(\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    azure_ad_token_provider=token_provider,\n",
    ")\n",
    "\n",
    "# Initialize the credential and client\n",
    "embedding_client = AzureOpenAI(\n",
    "        api_version=\"2024-02-01\",\n",
    "        azure_endpoint=embedding_endpoint,\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "\n",
    "# Define RAG functions \n",
    "def retrieve_documents(query, top_k):\n",
    "\n",
    "\n",
    "    embedding_response = embedding_client.embeddings.create(model=EMBEDDING_MODEL_NAME, input=query)\n",
    "    search_client = SearchClient(\n",
    "                                endpoint=AZURE_SEARCH_ENDPOINT, \n",
    "                                index_name=index_name, \n",
    "                                 credential=credential\n",
    "                                 )\n",
    "    query_vector = embedding_response.data[0].embedding\n",
    "    vector_query = VectorizedQuery(\n",
    "    vector=query_vector,\n",
    "    k_nearest_neighbors=top_k,\n",
    "    fields=\"content_vector\"  # The name of the vector field in ai search index\n",
    "    )\n",
    "    results = search_client.search(\n",
    "        select=[\"source\",\"page_title\",\"page_subtitle\",\"page_subtitle\", \"content\"],\n",
    "        vector_queries=[vector_query],\n",
    "        top=top_k)\n",
    "    \n",
    "    return [{\"source\": res[\"source\"], \"page_title\": res[\"page_title\"], \"page_subtitle\": res[\"page_subtitle\"], \"content\": res[\"content\"]} for res in results]\n",
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "   \n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in retrieved_docs])\n",
    "    system_message = \"You are an intelligent assistant...\"\n",
    "    user_message = f\"CONTEXT:\\n---\\n{context}\\n---\\nQUESTION: {query}\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": user_message}]\n",
    "    chat_response = aoai_client.chat.completions.create(model=chat_model_deployment, \n",
    "                                                        messages=messages, \n",
    "                                                        temperature=0.1)\n",
    "    final_answer = chat_response.choices[0].message.content\n",
    "\n",
    "    token_usage = chat_response.usage\n",
    "    return chat_response.choices[0].message.content, user_message,token_usage # Return the prompt too\n",
    "\n",
    "\n",
    "@mlflow.trace()\n",
    "def run_rag_pipeline(query, top_k):\n",
    "    \"\"\"\n",
    "    This function executes the full RAG pipeline and is traced by MLflow.\n",
    "    \"\"\"\n",
    "    #Retrieval Step as a Span\n",
    "    with mlflow.start_span(\"retrieval\") as span:\n",
    "        start_time = time.time()\n",
    "        documents = retrieve_documents(query, top_k)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        span.set_inputs({\"query\": query, \"top_k\": top_k})\n",
    "        span.set_outputs({\"documents\": documents})\n",
    "        span.set_attribute(\"duration_sec\", round(retrieval_time, 2))\n",
    "\n",
    "    # Generation Step as a Span\n",
    "    with mlflow.start_span(\"generation\") as span:\n",
    "        start_time = time.time()\n",
    "        final_answer, augmented_prompt,token_usage = generate_answer(query, documents)\n",
    "        generation_time = time.time() - start_time\n",
    "        \n",
    "        span.set_inputs({\"prompt\": augmented_prompt})\n",
    "        span.set_outputs({\"answer\": final_answer})\n",
    "        span.set_attribute(\"duration_sec\", round(generation_time, 2))\n",
    "        \n",
    "        if token_usage:\n",
    "            span.set_attribute(\"prompt_tokens\", token_usage.prompt_tokens)\n",
    "            span.set_attribute(\"completion_tokens\", token_usage.completion_tokens)\n",
    "            span.set_attribute(\"total_tokens\", token_usage.total_tokens)\n",
    "    return final_answer, documents, augmented_prompt,generation_time,retrieval_time,token_usage\n",
    "\n",
    "# Define the Delta table used as the source for the RAG knowledge base\n",
    "SOURCE_DELTA_TABLE = \"rag.development.md_chunks\"\n",
    "\n",
    "#Run and Track the Rag experiment \n",
    "user_query = \"What are ragamuffins\"\n",
    "top_k_value = 5\n",
    "\n",
    "with mlflow.start_run(run_name=\"RAG test\") as run:\n",
    "    print(f\"🚀 Starting MLflow run: {run.info.run_name}\")\n",
    "\n",
    "    # 2. 🔗 Load the Delta table as an MLflow Dataset, providing the version\n",
    "    print(f\"Linking source dataset: {SOURCE_DELTA_TABLE}\")\n",
    "    source_dataset = mlflow.data.load_delta(\n",
    "        table_name=SOURCE_DELTA_TABLE,\n",
    "        version=latest_version  # Pass the version here\n",
    "    )\n",
    "\n",
    "    # 3. Log the dataset as an input to the run for traceability\n",
    "    mlflow.log_input(source_dataset, context=\"source_documents\")\n",
    "\n",
    "    mlflow.log_params({\n",
    "        \"api_version\": OPENAI_API_VERSION,\n",
    "        \"top_k\": top_k_value,\n",
    "        \"search_text\": user_query,\n",
    "        \"embedding_model\": embedding_model_name,\n",
    "        \"search_index_name\": index_name,\n",
    "        \"search_type\": \"hnsw\",\n",
    "        \"embedding_dimensions\": EMBEDDING_DIMENSIONS,\n",
    "        \"chunk_overlap\": overlap,\n",
    "        \"chat_model\": chat_model_deployment\n",
    "    })\n",
    "\n",
    "    # Execute the traced pipeline\n",
    "    final_answer, documents, augmented_prompt, retrieval_time, generation_time,token_usage = run_rag_pipeline(\n",
    "        query=user_query, \n",
    "        top_k=top_k_value\n",
    "    )\n",
    "    metrics_to_log = {\n",
    "        \"retrieval_time_sec\": round(retrieval_time, 2),\n",
    "        \"generation_time_sec\": round(generation_time, 2),\n",
    "        \"total_time_sec\": round(retrieval_time + generation_time, 2)\n",
    "    }\n",
    "    if token_usage:\n",
    "        metrics_to_log[\"prompt_tokens\"] = token_usage.prompt_tokens\n",
    "        metrics_to_log[\"completion_tokens\"] = token_usage.completion_tokens\n",
    "        metrics_to_log[\"total_tokens\"] = token_usage.total_tokens\n",
    "        \n",
    "    mlflow.log_metrics(metrics_to_log)\n",
    "    # You can still log overall metrics and artifacts if you wish\n",
    "    rag_data = {\n",
    "        \"prompt\": [user_query],\n",
    "        \"prompt with context\": [augmented_prompt],\n",
    "        \"final_answer\": [final_answer],\n",
    "        \"retrieved_documents\": [str(documents)] # Convert list of docs to string for table storage\n",
    "    }\n",
    "    mlflow.log_table(data=pd.DataFrame(rag_data), artifact_file=\"rag_results.json\")\n",
    "    print(\"✅ MLflow run and trace completed.\")\n",
    "    print(f\"\\nFinal Answer:\\n{final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdbf6b8f-0570-4133-837f-cd5814e6daff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### assesment and review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7947a4-b1e7-4169-acdb-fd9a87312f62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20ffdf8e-0b8d-4a61-bb38-8c147d0486c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "export traces to a dataset"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.genai.datasets\n",
    "import time\n",
    "\n",
    "\n",
    "# 1. Create an evaluation dataset\n",
    "\n",
    "# Replace with a Unity Catalog schema where you have CREATE TABLE permission\n",
    "uc_schema = \"rag.development\"\n",
    "# This table will be created in the above UC schema\n",
    "evaluation_dataset_table_name = \"muffin_generation_eval\"\n",
    "\n",
    "eval_dataset = mlflow.genai.datasets.create_dataset(\n",
    "    uc_table_name=f\"{uc_schema}.{evaluation_dataset_table_name}\",\n",
    ")\n",
    "print(f\"Created evaluation dataset: {uc_schema}.{evaluation_dataset_table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "openai",
     "azure-identity"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6578920358797710,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks RAG",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
